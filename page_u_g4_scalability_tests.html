<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ug4: Scalability Tests</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="extra_stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ug4
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('page_u_g4_scalability_tests.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Scalability Tests </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#secScalabilityTestsGeneral">General Practical Information about Scalability Tests</a></li>
<li class="level1"><a href="#secScalabilityTestsSpecific">Specific Information about Scalability Tests</a><ul><li class="level2"><a href="#secHierarchicalRedistribution">Hierarchical Redistribution</a></li>
<li class="level2"><a href="#secTopology_aware_mapping_of_mpi_processes">Mapping of MPI processes</a></li>
<li class="level2"><a href="#secUtilities_for_scalability_tests">Utilities for Scalability Tests</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><hr  />
 <h1><a class="anchor" id="secScalabilityTestsGeneral"></a>
General Practical Information about Scalability Tests</h1>
<hr  />
<ul>
<li>
First of all you have to <b>enable profiling</b> by configuring your build with <em>CMake</em> (cf. <a class="el" href="page_u_g4_profiling.html#secEnableProfiling">Enable Profiling in ug4</a>): <div class="fragment"><div class="line"><a class="code" href="unit__tests_8doxygen.html#af700cfda4d235180bc73979f77d01cca">cmake</a> -DPROFILER=ON [-DPROFILE_PCL=ON] ..</div>
<div class="ttc" id="aunit__tests_8doxygen_html_af700cfda4d235180bc73979f77d01cca"><div class="ttname"><a href="unit__tests_8doxygen.html#af700cfda4d235180bc73979f77d01cca">cmake</a></div><div class="ttdeci">cmake</div><div class="ttdef"><b>Definition:</b> unit_tests.doxygen:198</div></div>
</div><!-- fragment -->  </li>
<li>
<p class="startli">Then, to get "clean" timing measurements, you should do a <b>release build</b>, i.e. </p><div class="fragment"><div class="line"><a class="code" href="unit__tests_8doxygen.html#af700cfda4d235180bc73979f77d01cca">cmake</a> -DEBUG=OFF ..</div>
</div><!-- fragment --><p class="endli">(Obviously the above configurations can be performed by only one <code>cmake</code> command.)  </p>
</li>
<li>
Furthermore, since generation of output can be very time consuming for jobs with a very large number of MPI processes its highly recommended to <b>turn off any output generation</b> by disabling debug writers and any other print/write operations for saving large data to files (vectors and matrices, refined grids, partition maps, solutions ...).  </li>
<li>
<p class="startli">Timing measurements are only useful at points where processes are synchronised, e.g. after computing of global norms, after performing <code>pcl::Synchronize()</code>, <code><a class="el" href="group__pcl.html#ga944eeb7a85cdf4e14beba56bf4729e3e">pcl::AllProcsTrue()</a></code> ... </p>
<p class="endli"></p>
</li>
<li>
<p class="startli">For weak scalability e.g. of <em>GMG</em> : Check if the number of iterations is constant over all problem sizes. </p>
<p class="endli"></p>
</li>
</ul>
<hr  />
 <h1><a class="anchor" id="secScalabilityTestsSpecific"></a>
Specific Information about Scalability Tests</h1>
<h2><a class="anchor" id="secHierarchicalRedistribution"></a>
Hierarchical Redistribution</h2>
<p>The approach of (re)distributing the grid to all MPI processes involved in a simulation run in a hierarchical fashion turned out to be essential for a good performance of large jobs (running with &gt;= 1024 PE) on <em>JuGene</em> (see <a class="el" href="page_u_g4_parallel_ju_gene.html#secWorking_with_ug4_on_JuGene">Working with ug4 on JuGene</a>).</p>
<ul>
<li>
Example (applicable in a <em>LoadLeveler</em> script on <em>JuGene</em>): <div class="fragment"><div class="line">mpirun -np   1024 -exe ./ugshell -mode VN -mapfile TXYZ -args <span class="stringliteral">&quot;&lt;other ug4 args&gt; -numPreRefs 3 -numRefs 10 -hRedistFirstLevel 5 -hRedistStepSize 100 -hRedistNewProcsPerStep  16&quot;</span></div>
</div><!-- fragment --> Or (a smaller, not very reasonable one) on <em>cekon:</em> <div class="fragment"><div class="line">salloc -n 64 mpirun ./ugshell -<a class="code" href="command__line__util_8lua.html#a340d3719e63a0a3a4f4429ef8c8b55ea">ex</a> ../apps/scaling_tests/modular_scalability_test.lua -numPreRefs 1 -hRedistFirstLevel 4 -hRedistStepSize 2 -hRedistNewProcsPerStep  4 -<a class="codeRef" href="../plugins/example__free__surface_2evaluate_8lua.html#ad2464918eb8162cae4af3f470845f0d9">numRefs</a> 8</div>
<div class="ttc" id="acommand__line__util_8lua_html_a340d3719e63a0a3a4f4429ef8c8b55ea"><div class="ttname"><a href="command__line__util_8lua.html#a340d3719e63a0a3a4f4429ef8c8b55ea">ex</a></div><div class="ttdeci">parameterString ex</div><div class="ttdoc">Executes the specified script.</div><div class="ttdef"><b>Definition:</b> command_line_util.lua:350</div></div>
<div class="ttc" id="aexample__free__surface_2evaluate_8lua_html_ad2464918eb8162cae4af3f470845f0d9"><div class="ttname"><a href="../plugins/example__free__surface_2evaluate_8lua.html#ad2464918eb8162cae4af3f470845f0d9">numRefs</a></div><div class="ttdeci">parameterNumber numRefs</div></div>
</div><!-- fragment -->  </li>
<li>
Parameters that control hierarchical redistribution: <ul>
<li>
<code>-numPreRefs</code> (as usual): level where the grid is <em>distributed</em> the first time.  </li>
<li>
<code>-numRefs</code> (as usual): toplevel of the grid hierarchy.  </li>
<li>
<code>-hRedistFirstLevel</code> (default -1): first level where grid is <em>re</em>distributed (default -1, i.e, hierarchical redistribution is deactivated).  </li>
<li>
<code>-hRedistStepSize</code> (default 1): Specifies after how much further refinements the grid will be redistributed again.  </li>
<li>
<code>-hRedistNewProcsPerStep</code> (default: 2<sup>dim</sup>): Number of MPI processes ("target procs") in a redistribution step to which each processor who already has received its part of the grid redistributes it.  </li>
<li>
<code>-hRedistMaxSteps</code> (default: 1000000000; not used in the example above): Limits the number of redistribution steps (to avoid useless redistributions involving only a few processes at the "end of the 
        hierarchy").  </li>
</ul>
The following inequality must apply: <code>numPreRefs &lt; hRedistFirstLevel &lt; numRefs</code>.  </li>
<li>
<p class="startli">Sketch of the algorithm: </p><ol>
<li>
At first the (pristine) grid (as defined by the grid file) will be refined <code>numPreRef</code> times =&gt; toplevel of the grid hierarchy is now level <code>numPreRef</code>.  </li>
<li>
At this level the grid will be distributed to <code>k</code> MPI processes (<code>k</code> will be explained below).  </li>
<li>
Now the grid will be further refined until level <code>hRedistFirstLevel</code> is reached.  </li>
<li>
There the grid will be <em>re</em>distributed to another <code>hRedistNumProcsPerStep</code> MPI processes for every process which already has received a part of the grid.  </li>
<li>
Then the grid will be refined at most <code>hRedistStepSize</code> times:<br  />
 If <code>numRefs</code> refinement steps, and also the number of redistribution steps controlled by <code>-hRedistMaxSteps</code> are not yet reached, go to 4. (else: finished).  </li>
</ol>
<p>Now all MPI processes of the simulation run have their part of the grid. To make things clear: </p><ul>
<li>
After <code>numPreRefs</code> refinement steps the grid will be <em>distributed</em>, and  </li>
<li>
after the grid is already distributed to some processes it will be <em>re</em>distributed  </li>
<li>
at level(s) <code>hRedistFirstLevel + i * hRedistStepSize</code><br  />
 (<code>numPreRefs + hRedistFirstLevel + i * hRedistStepSize &lt; numRefs</code>; <code>0 &lt;= i &lt; hRedistMaxSteps</code>),<br  />
 until at the end all MPI processes have received a portion of the grid.  </li>
</ul>
<p>So, all parameters with name part <code>"Redist"</code> refer to the redistribution of an already distributed grid. <br  />
 The number of processes <code>k</code> of the first distribution step is determined by the (total) number of MPI processes, <code>numProcs</code>, on one side, and the other redistribution parameters on the other side, starting "from top" (i.e. top most redistribution level) "to bottom" (first distribution step):<br  />
 <code>numProcs / hRedistNumProcsPerStep</code> is the number of target procs to which the grid is distributed in the second last redistribution step,<br  />
 <code>numProcs / hRedistNumProcsPerStep / hRedistNumProcsPerStep</code> the number of target procs in the third last redistribution step (or the first distribution step, if only one redistribution step is performed) etc.</p>
<p class="endli"></p>
</li>
<li>
A test of the paramaters for hierarchical redistribution can be performed with the LUA script <code>parameter_test.lua</code>, e.g. <div class="fragment"><div class="line">./ugshell -<a class="code" href="command__line__util_8lua.html#a340d3719e63a0a3a4f4429ef8c8b55ea">ex</a> ../apps/scaling_tests/parameter_test.lua -numPreRefs 3 -<a class="codeRef" href="../plugins/example__free__surface_2evaluate_8lua.html#ad2464918eb8162cae4af3f470845f0d9">numRefs</a> 10 -hRedistFirstLevel 5 -hRedistStepSize 100 -hRedistNewProcsPerStep 16 -numProcs 1024</div>
</div><!-- fragment --> Or the dry run for the smaller job on <em>cekon</em> above: <div class="fragment"><div class="line">./ugshell -<a class="code" href="command__line__util_8lua.html#a340d3719e63a0a3a4f4429ef8c8b55ea">ex</a> ../apps/scaling_tests/parameter_test.lua -numPreRefs 1 -hRedistFirstLevel 4 -hRedistStepSize 2 -hRedistNewProcsPerStep  4 -<a class="codeRef" href="../plugins/example__free__surface_2evaluate_8lua.html#ad2464918eb8162cae4af3f470845f0d9">numRefs</a> 8 -numProcs 64</div>
</div><!-- fragment --> I.e. since the run is a serial run one has just to specify the number of MPI processes in addition to the distribution parameters.  </li>
<li>
Please note that this is only a "dry run": The script basically processes the same algorithm (<code>ddu.PrintSteps()</code>) as the one that actually carries out the (re)distribution (<code>ddu.RefineAndDistributeDomain()</code>; cf. <code><a class="el" href="domain__distribution__util_8lua.html">domain_distribution_util.lua</a></code>).  </li>
</ul>
<p>Please note that hierarchical redistribution is not compatible with "grid 
distribution type" (<code>distributionType</code>) <code>"grid2d"</code> (see <a class="el" href="page_u_g4_scalability_tests.html#secTopology_aware_mapping_of_mpi_processes">Mapping of MPI processes</a>). In the moment (march 2012) also grid distribution type <code>"metis"</code> is unsupported.</p>
<p>See also e.g. <code>ll_scale_gmg.x</code> (in <code>scripts/shell/</code>) for usage examples (specifically to <em>JuGene</em>, but also in general).</p>
<h2><a class="anchor" id="secTopology_aware_mapping_of_mpi_processes"></a>
Mapping of MPI processes</h2>
<p>"Topology aware mapping" of MPI processes to nodes / cores with respect to the network topology of the parallel machine on which a parallel job is run might be important one day.</p>
<ul>
<li>
Parameter <code>-distType</code>: Available values: "grid2d", "bisect", "metis"  </li>
</ul>
<h2><a class="anchor" id="secUtilities_for_scalability_tests"></a>
Utilities for Scalability Tests</h2>
<ul>
<li>
There exist some LUA scripts specifically tuned for timing measurements (pathes relavtive to ug4's main directory): <ul>
<li>
For the Laplace problem: <code>apps/scaling_tests/modular_scalability_test.lua</code>.  </li>
<li>
For the Elder problem: <code>apps/d3f/elder_scalability_test.lua</code>.  </li>
</ul>
</li>
<li>
For a (quiet convenient) <b>analysis</b> of the profiling results of several simulation runs there also exist a special <b>analyzer script</b>: <code><a class="el" href="scaling__analyzer_8lua.html" title="lua script to compare profiling outputs from different runs of one problem">scripts/tools/scaling_analyzer.lua</a></code>. <ol>
<li>
To get log files of your simulation runs use the <code>-ugshell</code> parameter <code>-logtofile</code>. This is of course not necessary if logfiles are automatically created by the resource manager (e.g., on <a class="el" href="page_u_g4_parallel_ju_gene.html#secJuGene">General Information about JuGene</a>).  </li>
<li>
In the list variable <code>inFiles</code> of the analyzer script enter the names of the logfiles of all runs which profiling results should be analysed (edit a local copy).  </li>
<li>
Then execute (with the stand-alone LUA interpreter; see <a class="el" href="page_additional_software.html#secInstallLUA">Installation of LUA (optional)</a> for installation if necessary): <div class="fragment"><div class="line">lua my_scaling_analyzer.lua</div>
</div><!-- fragment --> or, redirecting the output to a file : <div class="fragment"><div class="line">lua my_scaling_analyzer.lua &gt; jugene_ug4-static_laplace-2d_gmg_weak-scaling_pe4-256k_rev4354.txt</div>
</div><!-- fragment --> If <code>ugshell</code> is executable on the machine used for this analysis (which is <em>not</em> the case if you are working on a login node of e.g. <em>JuGene</em>), one can also execute <code>ugshell -ex jugene/scaling_analyzer.lua</code> (adapt file pathes in <code>inFiles</code> relative to <code>ugshell</code>).  </li>
</ol>
</li>
<li>
See also the <code>util.printStats(stats)</code> functionality, e.g. in <code>apps/scaling_tests/modular_scalability_test.lua</code>.  </li>
</ul>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="index.html">ug4 - Documentation</a></li><li class="navelem"><a class="el" href="page_u_g4_usage.html">Usage</a></li><li class="navelem"><a class="el" href="page_tutorials.html">Tutorials</a></li>
    <li class="footer">Generated on Mon Aug 21 2023 00:51:28 for ug4 by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1 </li>
  </ul>
</div>
</body>
</html>
